{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Memory in LLMs"
      ],
      "metadata": {},
      "id": "01a8b5c0-87cb-4302-8e3c-dc809d0039fb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous Notebooks, we successfully explored how OpenAI models can enhance the results from Azure AI Search queries. \n",
        "\n",
        "However, we have yet to discover how to engage in a conversation with the LLM. With [Bing Chat](http://chat.bing.com/), for example, this is possible, as it can understand and reference the previous responses.\n",
        "\n",
        "There is a common misconception that LLMs (Large Language Models) have memory. This is not true. While they possess knowledge, they do not retain information from previous questions asked to them.\n",
        "\n",
        "In this Notebook, our goal is to illustrate how we can effectively \"endow the LLM with memory\" by employing prompts and context."
      ],
      "metadata": {},
      "id": "a2f73380-6395-4e9f-9c83-3f47a5d7e292"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory, CosmosDBChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from operator import itemgetter\n",
        "from typing import List\n",
        "\n",
        "from IPython.display import Markdown, HTML, display  \n",
        "\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "\n",
        "#custom libraries that we will use later in the app\n",
        "from common.utils import CustomAzureSearchRetriever, get_answer\n",
        "from common.prompts import DOCSEARCH_PROMPT\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import logging\n",
        "\n",
        "# Get the root logger\n",
        "logger = logging.getLogger()\n",
        "# Set the logging level to a higher level to ignore INFO messages\n",
        "logger.setLevel(logging.WARNING)"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1709737185617
        }
      },
      "id": "733c782e-204c-47d0-8dae-c9df7091ab23"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1709737187504
        }
      },
      "id": "6bc63c55-a57d-49a7-b6c7-0f18bca8199e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's start with the basics\n",
        "Let's use a very simple example to see if the GPT model of Azure OpenAI have memory. We again will be using langchain to simplify our code "
      ],
      "metadata": {},
      "id": "3dc72b22-11c2-4df0-91b8-033d01829663"
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION = \"Tell me some use cases for reinforcement learning\"\n",
        "FOLLOW_UP_QUESTION = \"What was my prior question?\""
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1709737191597
        }
      },
      "id": "3eef5dc9-8b80-4085-980c-865fa41fa1f6"
    },
    {
      "cell_type": "code",
      "source": [
        "COMPLETION_TOKENS = 1500\n",
        "# Create an OpenAI instance\n",
        "llm = AzureChatOpenAI(deployment_name=os.environ[\"GPT35_DEPLOYMENT_NAME\"], \n",
        "                      temperature=0.5, max_tokens=COMPLETION_TOKENS)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1709737195682
        }
      },
      "id": "a00181d5-bd76-4ce4-a256-75ac5b58c60f"
    },
    {
      "cell_type": "code",
      "source": [
        "# We create a very simple prompt template, just the question as is:\n",
        "output_parser = StrOutputParser()\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an assistant that give thorough responses to users.\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1709737197496
        }
      },
      "id": "9502d0f1-fddf-40d1-95d2-a1461dcc498a"
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see what the GPT model responds\n",
        "chain = prompt | llm | output_parser\n",
        "response_to_initial_question = chain.invoke({\"input\": QUESTION})\n",
        "display(Markdown(response_to_initial_question))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with an environment and receiving feedback in the form of rewards or punishments. Here are some common use cases for reinforcement learning:\n\n1. Game playing: Reinforcement learning has been successfully applied to various games, such as chess, Go, and video games. Agents can learn to play games at a high level by exploring different strategies and refining their decision-making abilities.\n\n2. Robotics: Reinforcement learning can be used to train robots to perform complex tasks, such as grasping objects, walking, or even flying drones. By interacting with the environment, robots can learn to optimize their actions to achieve desired goals.\n\n3. Autonomous vehicles: Reinforcement learning can be used to train self-driving cars to make decisions in real-time, such as lane changing, merging, and navigating through complex traffic scenarios. Agents can learn to optimize their driving behavior based on safety, efficiency, and passenger comfort.\n\n4. Resource management: Reinforcement learning can be applied to optimize the allocation of resources in various domains, such as energy grids, transportation systems, or supply chains. Agents can learn to make decisions that maximize efficiency, minimize costs, or reduce waste.\n\n5. Personalized recommendation systems: Reinforcement learning can be used to build recommendation systems that learn from user feedback. By exploring different recommendations and observing user responses, the system can learn to make personalized recommendations that maximize user satisfaction.\n\n6. Healthcare: Reinforcement learning can be applied to healthcare settings, such as optimizing treatment plans for chronic diseases, personalized drug dosage recommendations, or adaptive clinical trial designs. Agents can learn to make decisions that maximize patient outcomes while considering individual characteristics and medical constraints.\n\nThese are just a few examples, and the potential use cases for reinforcement learning are vast and diverse. The key is to identify domains where decision-making is crucial and where the agent can interact with an environment to learn and improve its behavior over time."
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1709737204995
        }
      },
      "id": "c5c9903e-15c7-4e05-87a1-58e5a7917ba2"
    },
    {
      "cell_type": "code",
      "source": [
        "#Now let's ask a follow up question\n",
        "printmd(chain.invoke({\"input\": FOLLOW_UP_QUESTION}))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "I'm sorry, but as an AI language model, I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. Therefore, I don't have knowledge of your prior question or any other personal data. My primary function is to provide information and answer questions to the best of my knowledge and abilities. If there's anything specific you'd like assistance with, feel free to ask!"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1709737219686
        }
      },
      "id": "99acaf3c-ce68-4b87-b24a-6065b15ff9a8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, it doesn't remember what it just responded, sometimes it responds based only on the system prompt, or just randomly. This proof that the LLM does NOT have memory and that we need to give the memory as a a conversation history as part of the prompt, like this:"
      ],
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "id": "a3e1c143-c95f-4566-a8b4-af8c42f08dd2"
    },
    {
      "cell_type": "code",
      "source": [
        "hist_prompt = ChatPromptTemplate.from_template(\n",
        "\"\"\"\n",
        "    {history}\n",
        "    Human: {question}\n",
        "    AI:\n",
        "\"\"\"\n",
        ")\n",
        "chain = hist_prompt | llm | output_parser"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1709737226723
        }
      },
      "id": "0946ce71-6285-432e-b011-9c2dc1ba7b8a"
    },
    {
      "cell_type": "code",
      "source": [
        "Conversation_history = \"\"\"\n",
        "Human: {question}\n",
        "AI: {response}\n",
        "\"\"\".format(question=QUESTION, response=response_to_initial_question)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1709737228945
        }
      },
      "id": "6d088e51-e5eb-4143-b87d-b2be429eb864"
    },
    {
      "cell_type": "code",
      "source": [
        "printmd(chain.invoke({\"history\":Conversation_history, \"question\": FOLLOW_UP_QUESTION}))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "Your prior question was \"Tell me some use cases for reinforcement learning\"."
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1709737230559
        }
      },
      "id": "d99e34ad-5539-44dd-b080-3ad05efd2f01"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bingo!**, so we now know how to create a chatbot using LLMs, we just need to keep the state/history of the conversation and pass it as context every time"
      ],
      "metadata": {},
      "id": "045e5af6-55d6-4353-b3f6-3275c95db00a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now that we understand the concept of memory via adding history as a context, let's go back to our GPT Smart Search engine"
      ],
      "metadata": {},
      "id": "eafd1694-0077-4aa8-bd01-e9f763ce36a3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "From Langchain website:\n",
        "    \n",
        "A memory system needs to support two basic actions: reading and writing. Recall that every chain defines some core execution logic that expects certain inputs. Some of these inputs come directly from the user, but some of these inputs can come from memory. A chain will interact with its memory system twice in a given run.\n",
        "\n",
        "    AFTER receiving the initial user inputs but BEFORE executing the core logic, a chain will READ from its memory system and augment the user inputs.\n",
        "    AFTER executing the core logic but BEFORE returning the answer, a chain will WRITE the inputs and outputs of the current run to memory, so that they can be referred to in future runs.\n",
        "    \n",
        "So this process adds delays to the response, but it is a necessary delay :)"
      ],
      "metadata": {},
      "id": "9787ffb6-2b11-4b03-92fc-9443cd1f2ab9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://python.langchain.com/assets/images/memory_diagram-0627c68230aa438f9b5419064d63cbbc.png)"
      ],
      "metadata": {},
      "id": "f36e8f14-e566-4ae9-a7d4-6dee7f469dad"
    },
    {
      "cell_type": "code",
      "source": [
        "index1_name = \"cogsrch-index-files\"\n",
        "index2_name = \"cogsrch-index-csv\"\n",
        "index3_name = \"cogsrch-index-books\"\n",
        "indexes = [index1_name, index2_name, index3_name]"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1709737255677
        }
      },
      "id": "ef9f459b-e8b8-40b9-a94d-80c079968594"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize our custom retriever \n",
        "retriever = CustomAzureSearchRetriever(indexes=indexes, topK=10, reranker_threshold=1)"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1709737307520
        }
      },
      "id": "b01852c2-6192-496c-adff-4270f9380469"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you check closely in prompts.py, there is an optional variable in the `DOCSEARCH_PROMPT` called `history`. Now it is the time to use it. It is basically a place holder were we will inject the conversation in the prompt so the LLM is aware of it before it answers."
      ],
      "metadata": {},
      "id": "633937e8-18e6-43f2-b4d5-fc36157a4d97"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now let's add memory to it:**"
      ],
      "metadata": {},
      "id": "035fa6e6-226c-400f-a504-30255385f43b"
    },
    {
      "cell_type": "code",
      "source": [
        "store = {} # Our first memory will be a dictionary in memory\n",
        "\n",
        "# We have to define a custom function that takes a session_id and looks somewhere\n",
        "# (in this case in a dictionary in memory) for the conversation\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1709737319110
        }
      },
      "id": "3c8c9381-08d0-4808-9ab1-78156ca1be6e"
    },
    {
      "cell_type": "code",
      "source": [
        "# We use our original chain with the retriever but removing the StrOutputParser\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | retriever, \n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        \"history\": itemgetter(\"history\")\n",
        "    }\n",
        "    | DOCSEARCH_PROMPT\n",
        "    | llm\n",
        ")\n",
        "\n",
        "## Then we pass the above chain to another chain that adds memory to it\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"question\",\n",
        "    history_messages_key=\"history\",\n",
        ") | output_parser"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1709737320535
        }
      },
      "id": "48ff51e1-2b1e-4c67-965d-1c2e2f55e005"
    },
    {
      "cell_type": "code",
      "source": [
        "# This is where we configure the session id\n",
        "config={\"configurable\": {\"session_id\": \"abc123\"}}"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1709737321458
        }
      },
      "id": "0e582915-243f-42cb-bb1e-c35a20ee0b9f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice below, that we are adding a `history` variable in the call. This variable will hold the chat historywithin the prompt."
      ],
      "metadata": {},
      "id": "9ff493b1-b133-4880-a040-e80f7460e7af"
    },
    {
      "cell_type": "code",
      "source": [
        "printmd(chain_with_history.invoke({\"question\": QUESTION}, config=config))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "Reinforcement learning (RL) has been applied to various domains and has shown promising results in solving complex problems. Here are some notable use cases for reinforcement learning:\n\n1. **Game Playing**: RL has been successfully applied to various games, such as chess, Go, and poker. For example, AlphaGo, developed by DeepMind, used RL to defeat world champion Go players. RL algorithms can learn optimal strategies through trial and error, resulting in superhuman performance in game playing.\n\n2. **Robotics**: RL has been used to train robots to perform complex tasks, such as grasping objects, walking, and flying. RL enables robots to learn from their interactions with the environment and improve their performance over time. This has applications in industrial automation, healthcare, and autonomous vehicles.\n\n3. **Recommendation Systems**: RL can be used to personalize recommendations for users in various domains, such as e-commerce, entertainment, and advertising. By learning from user feedback and interactions, RL algorithms can optimize recommendations to maximize user satisfaction and engagement.\n\n4. **Resource Management**: RL can be applied to optimize resource allocation and scheduling in various industries, such as transportation, logistics, and energy. RL algorithms can learn to make decisions in dynamic environments to maximize efficiency and minimize costs.\n\n5. **Finance and Trading**: RL has been used in financial markets to develop trading strategies and make investment decisions. RL algorithms can learn to exploit patterns in market data and adapt to changing market conditions, leading to improved trading performance.\n\n6. **Healthcare**: RL has the potential to optimize treatment plans and decision-making in healthcare. It can be used to personalize treatment strategies for patients, optimize scheduling in hospitals, and improve disease diagnosis and prediction.\n\n7. **Autonomous Agents**: RL is crucial in developing autonomous agents that can learn to navigate and interact with their environment. This has applications in autonomous vehicles, drones, and virtual assistants.\n\nThese are just a few examples of the wide range of use cases for reinforcement learning. RL continues to be an active area of research and development, with potential applications in many other domains."
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1709737333503
        }
      },
      "id": "d91a7ff4-6148-459d-917c-37302805dd09"
    },
    {
      "cell_type": "code",
      "source": [
        "# Remembers\n",
        "printmd(chain_with_history.invoke({\"question\": FOLLOW_UP_QUESTION},config=config))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "Your prior question was: \"Tell me some use cases for reinforcement learning.\""
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1709737372434
        }
      },
      "id": "25dfc233-450f-4671-8f1c-0b446e46f048"
    },
    {
      "cell_type": "code",
      "source": [
        "# Remembers\n",
        "printmd(chain_with_history.invoke({\"question\": \"Thank you! Good bye\"},config=config))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "You're welcome! If you have any more questions in the future, feel free to ask. Goodbye and take care!"
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1709737377942
        }
      },
      "id": "c67073c2-9a82-4e44-a9e2-48fe868c1634"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using CosmosDB as persistent memory\n",
        "\n",
        "In previous cell we have added local RAM memory to our chatbot. However, it is not persistent, it gets deleted once the app user's session is terminated. It is necessary then to use a Database for persistent storage of each of the bot user conversations, not only for Analytics and Auditing, but also if we wish to provide recommendations in the future. \n",
        "\n",
        "Here we will store the conversation history into CosmosDB for future auditing purpose.\n",
        "We will use a class in LangChain use CosmosDBChatMessageHistory"
      ],
      "metadata": {},
      "id": "87405173"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the function to retrieve the conversation\n",
        "\n",
        "def get_session_history(session_id: str, user_id: str) -> CosmosDBChatMessageHistory:\n",
        "    cosmos = CosmosDBChatMessageHistory(\n",
        "        cosmos_endpoint=os.environ['AZURE_COSMOSDB_ENDPOINT'],\n",
        "        cosmos_database=os.environ['AZURE_COSMOSDB_NAME'],\n",
        "        cosmos_container=os.environ['AZURE_COSMOSDB_CONTAINER_NAME'],\n",
        "        connection_string=os.environ['AZURE_COMOSDB_CONNECTION_STRING'],\n",
        "        session_id=session_id,\n",
        "        user_id=user_id\n",
        "        )\n",
        "\n",
        "    # prepare the cosmosdb instance\n",
        "    cosmos.prepare_cosmos()\n",
        "    return cosmos\n"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1709737383828
        }
      },
      "id": "d87cc7c6-5ef1-4492-b133-9f63a392e223"
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"question\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"user_id\",\n",
        "            annotation=str,\n",
        "            name=\"User ID\",\n",
        "            description=\"Unique identifier for the user.\",\n",
        "            default=\"\",\n",
        "            is_shared=True,\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"Unique identifier for the conversation.\",\n",
        "            default=\"\",\n",
        "            is_shared=True,\n",
        "        ),\n",
        "    ],\n",
        ") | output_parser"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1709737384706
        }
      },
      "id": "94f4179b-c1c7-49da-9c80-a42c275ed4d6"
    },
    {
      "cell_type": "code",
      "source": [
        "# This is where we configure the session id and user id\n",
        "random_session_id = \"session\"+ str(random.randint(1, 1000))\n",
        "ramdom_user_id = \"user\"+ str(random.randint(1, 1000))\n",
        "\n",
        "config={\"configurable\": {\"session_id\": random_session_id, \"user_id\": ramdom_user_id}}"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1709737385883
        }
      },
      "id": "8cf1f1f0-6e46-4136-9f33-4e46617b7d4f"
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "{'configurable': {'session_id': 'session155', 'user_id': 'user185'}}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1709737386728
        }
      },
      "id": "0b20c00c-4098-4970-84e5-f71ea7615c65"
    },
    {
      "cell_type": "code",
      "source": [
        "printmd(chain_with_history.invoke({\"question\": QUESTION}, config=config))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "Reinforcement learning (RL) has been applied to various domains and has shown promising results. Here are some use cases for reinforcement learning:\n\n1. **Game Playing**: RL has been successfully applied to game playing, where agents learn to make optimal decisions in complex game environments. For example, AlphaGo, developed by DeepMind, used RL to defeat world champion Go players.\n\n2. **Robotics**: RL has been used to train robots to perform complex tasks and navigate real-world environments. Robots can learn to manipulate objects, walk, and perform other physical actions through RL algorithms.\n\n3. **Autonomous Vehicles**: RL can be used to train autonomous vehicles to make decisions in dynamic and uncertain environments. Agents can learn to navigate traffic, make lane changes, and respond to unexpected situations.\n\n4. **Recommendation Systems**: RL can be used in personalized recommendation systems to learn user preferences and make recommendations. Agents can learn to optimize the selection of items, such as movies, music, or products, based on user feedback.\n\n5. **Resource Management**: RL can be applied to optimize resource allocation and management in various domains. For example, in energy management, RL can be used to optimize the scheduling of power generation and storage systems.\n\n6. **Healthcare**: RL can be used in healthcare for personalized treatment planning, drug dosage optimization, and disease management. Agents can learn to make treatment decisions based on patient data and optimize patient outcomes.\n\n7. **Finance**: RL can be applied to financial trading, portfolio management, and risk assessment. Agents can learn to make trading decisions based on market data and optimize investment strategies.\n\n8. **Supply Chain Management**: RL can be used to optimize inventory management, pricing, and logistics in supply chain operations. Agents can learn to make decisions that minimize costs and maximize efficiency.\n\n9. **Control Systems**: RL can be used in control systems to optimize the control policies of complex systems. For example, RL can be applied to optimize the control of power grids, manufacturing processes, or autonomous drones.\n\nThese are just a few examples of the diverse applications of reinforcement learning. RL has the potential to revolutionize various industries by enabling intelligent decision-making and optimization in complex and dynamic environments."
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1709737398805
        }
      },
      "id": "7e3c32f4-f883-4045-91f9-ca317c2d01fe"
    },
    {
      "cell_type": "code",
      "source": [
        "# Remembers\n",
        "printmd(chain_with_history.invoke({\"question\": FOLLOW_UP_QUESTION},config=config))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "Your prior question was: \"Tell me some use cases for reinforcement learning.\""
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1709737405542
        }
      },
      "id": "7e29643b-a531-4117-8e85-9c88a625cf02"
    },
    {
      "cell_type": "code",
      "source": [
        "# Remembers\n",
        "printmd(chain_with_history.invoke(\n",
        "    {\"question\": \"Can you tell me a one line summary of our conversation?\"},\n",
        "    config=config))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "In our conversation, we discussed various use cases for reinforcement learning, including game playing, robotics, autonomous vehicles, recommendation systems, resource management, healthcare, finance, supply chain management, and control systems."
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1709737493632
        }
      },
      "id": "50146f05-5ef6-484f-a8ec-9631643054f2"
    },
    {
      "cell_type": "code",
      "source": [
        "printmd(chain_with_history.invoke(\n",
        "    {\"question\": \"Thank you very much!\"},\n",
        "    config=config))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "You're welcome! If you have any more questions, feel free to ask."
          },
          "metadata": {}
        }
      ],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1709737500444
        }
      },
      "id": "8bc02369-904c-4063-93e1-fff24fe6a3ab"
    },
    {
      "cell_type": "code",
      "source": [
        "printmd(chain_with_history.invoke(\n",
        "    {\"question\": \"I do have one more question, why did you give me a one line summary?\"},\n",
        "    config=config))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "I provided a one-line summary as a concise overview of the main topic and points discussed in our conversation. It serves as a quick reference or summary of the key information covered."
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1709737505959
        }
      },
      "id": "87d60faa-1446-4c07-8970-0f9712c33b2f"
    },
    {
      "cell_type": "code",
      "source": [
        "printmd(chain_with_history.invoke(\n",
        "    {\"question\": \"why not 2?\"},\n",
        "    config=config))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "I apologize for not providing a two-line summary. I can certainly provide a two-line summary if you prefer. Here it is:\n\nIn our conversation, we discussed various use cases for reinforcement learning, including game playing, robotics, autonomous vehicles, recommendation systems, resource management, healthcare, finance, supply chain management, and control systems. Additionally, we touched upon the importance of a concise summary for quick reference."
          },
          "metadata": {}
        }
      ],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1709737515515
        }
      },
      "id": "cfe748aa-6116-4a7a-97e6-f1c680dd23ad"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's check our Azure CosmosDB to see the whole conversation\n"
      ],
      "metadata": {},
      "id": "cdc5ac98"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![CosmosDB Memory](./images/cosmos-chathistory.png)"
      ],
      "metadata": {},
      "id": "f5e30694-ae2a-47bb-a5c7-db51ecdbba1e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "##### Adding memory to our application allows the user to have a conversation, however this feature is not something that comes with the LLM, but instead, memory is something that we must provide to the LLM in form of context of the question.\n",
        "\n",
        "We added persitent memory using CosmosDB.\n",
        "\n",
        "We also can notice that the current chain that we are using is smart, but not that much. Although we have given memory to it, it searches for similar docs everytime, regardless of the input. This doesn't seem efficient, but regardless, we are very close to finish our first RAG-talk to your data Bot.\n",
        "\n",
        "\n",
        "## <u>Important Note</u>:<br>\n",
        "As we proceed, while all the code will remain compatible with GPT-3.5 models (1106 or newer), we highly recommend transitioning to GPT-4. Here's why:\n",
        "\n",
        "**GPT-3.5-Turbo** can be likened to a 7-year-old child. You can provide it with concise instructions, but it struggles sometimes to follow them accurately (not too reliable). Additionally, its limited \"memory\" (token context) can make sustained conversations challenging. Its response are also simple not deep.\n",
        "\n",
        "**GPT-4-Turbo** exhibits the capabilities of a 10-12-year-old child. It possesses enhanced reasoning skills, consistently adheres to instructions and its answers are beter. It has extended memory retention (larger context size) for instructions, and it excels at following them. Its responses are deep and thorough.\n"
      ],
      "metadata": {},
      "id": "6789cada-23a3-451a-a91a-0906ceb0bd14"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEXT\n",
        "We know now how to do a Smart Search Engine that can power a chatbot!! great!\n",
        "\n",
        "In the next notebook 6, we are going to build our first RAG bot. In order to do this we will introduce the concept of Agents."
      ],
      "metadata": {},
      "id": "c629ebf4-aced-45b7-a6a2-315810d37d48"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "f53a8f7a-5e28-4d5f-9a33-0a3be0536b0f"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}