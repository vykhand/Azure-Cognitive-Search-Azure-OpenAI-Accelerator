{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building our First RAG bot - Skill: talk to Search Engine"
      ],
      "metadata": {},
      "id": "76fbaf88-5952-47bf-a68c-85011e49b6de"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now all the building blocks to build our first Bot that \"talks with my data\". These blocks are:\n",
        "\n",
        "1) A well indexed hybrid (text and vector) engine with my data in chunks -> Azure AI Search\n",
        "2) A good LLM python framework to build LLM Apps -> LangChain\n",
        "3) Quality OpenAI GPT models that understand language and follow instructions -> GPT3.5 and GPT4\n",
        "4) A persisten memory database -> CosmosDB\n",
        "\n",
        "We are missing just one thing: **Agents**.\n",
        "\n",
        "In this Notebook we introduce the concept of Agents and we use it to build or first RAG bot."
      ],
      "metadata": {},
      "id": "967c3b06-c8a0-45db-be9a-974c762ba4b8"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import asyncio\n",
        "from typing import Dict, List\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import Optional, Type\n",
        "\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_core.runnables import ConfigurableField, ConfigurableFieldSpec\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory, CosmosDBChatMessageHistory\n",
        "from langchain.callbacks.manager import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun\n",
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from langchain.tools import BaseTool, StructuredTool, tool\n",
        "\n",
        "#custom libraries that we will use later in the app\n",
        "from common.utils import  GetDocSearchResults_Tool\n",
        "from common.prompts import AGENT_DOCSEARCH_PROMPT\n",
        "\n",
        "from IPython.display import Markdown, HTML, display  \n",
        "\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1709738825043
        }
      },
      "id": "b64f701d-5b9d-4c7c-b259-c2a515c75961"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1709738829589
        }
      },
      "id": "e4163af7-39d0-43b4-8dad-c13108d22a1d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing: Agents"
      ],
      "metadata": {},
      "id": "33836104-822e-4846-8b81-0de8e24838f1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The implementation of Agents is inspired by two papers: the [MRKL Systems](https://arxiv.org/abs/2205.00445) paper (pronounced â€˜miracleâ€™ ðŸ˜‰) and the [ReAct](https://arxiv.org/abs/2210.03629) paper.\n",
        "\n",
        "Agents are a way to leverage the ability of LLMs to understand and act on prompts. In essence, an Agent is an LLM that has been given a very clever initial prompt. The prompt tells the LLM to break down the process of answering a complex query into a sequence of steps that are resolved one at a time.\n",
        "\n",
        "Agents become really cool when we combine them with â€˜expertsâ€™, introduced in the MRKL paper. Simple example: an Agent might not have the inherent capability to reliably perform mathematical calculations by itself. However, we can introduce an expert - in this case a calculator, an expert at mathematical calculations. Now, when we need to perform a calculation, the Agent can call in the expert rather than trying to predict the result itself. This is actually the concept behind [ChatGPT Pluggins](https://openai.com/blog/chatgpt-plugins).\n",
        "\n",
        "In our case, in order to solve the problem \"How do I build a smart bot that talks to my data\", we need this REACT/MRKL approach, in which we need to instruct the LLM that it needs to use 'experts/tools' in order to read/load/understand/interact with a any particular source of data.\n",
        "\n",
        "Let's create then an Agent that interact with the user and uses a Tool to get the information from the Search engine."
      ],
      "metadata": {},
      "id": "16fc3d38-93f8-4a47-8125-d1bb9f529178"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We start first defining the Tool/Expert"
      ],
      "metadata": {},
      "id": "a7999a06-aff0-4d21-8be7-fe56c70082a8"
    },
    {
      "cell_type": "code",
      "source": [
        "index1_name = \"cogsrch-index-files\"\n",
        "index2_name = \"cogsrch-index-csv\"\n",
        "index3_name = \"cogsrch-index-books\"\n",
        "indexes = [index1_name, index2_name, index3_name]"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1709738832964
        }
      },
      "id": "a862366b-ce9e-44f8-9610-84ec568653ea"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to convert the Retreiver object into a Tool object (\"the expert\"). Check out the Tool `GetDocSearchResults_Tool` in `utils.py`"
      ],
      "metadata": {},
      "id": "077886c8-c5d0-481d-a5f9-f4becf60e0f9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Declare the tools the agent will use"
      ],
      "metadata": {},
      "id": "f73c6ca7-d93b-4961-b90a-08572cad78d8"
    },
    {
      "cell_type": "code",
      "source": [
        "topK=7\n",
        "tools = [GetDocSearchResults_Tool(indexes=indexes, k=5, reranker_th=1, sas_token=os.environ['BLOB_SAS_TOKEN'])]"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1709738835661
        }
      },
      "id": "4a0fd3a0-527c-42e3-a092-46e03d33bd07"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the prompt to use `AGENT_DOCSEARCH_PROMPT` - you can modify this in `prompts.py`! Check it out!"
      ],
      "metadata": {},
      "id": "f9cac295-8be5-4803-8342-6d4e48cd2294"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = AGENT_DOCSEARCH_PROMPT"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1709738841774
        }
      },
      "id": "a44f8df6-a68e-4215-99f3-10119f796c0c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the LLM to use"
      ],
      "metadata": {},
      "id": "5f3ddf18-3f3c-44b4-8af5-1437973da010"
    },
    {
      "cell_type": "code",
      "source": [
        "COMPLETION_TOKENS = 1500\n",
        "llm = AzureChatOpenAI(deployment_name=os.environ[\"GPT35_DEPLOYMENT_NAME\"], temperature=0.5, max_tokens=COMPLETION_TOKENS, streaming=True).configurable_alternatives(\n",
        "    ConfigurableField(id=\"model\"),\n",
        "    default_key=\"gpt35\",\n",
        "    gpt4=AzureChatOpenAI(deployment_name=os.environ[\"GPT4_DEPLOYMENT_NAME\"], temperature=0.5, max_tokens=COMPLETION_TOKENS, streaming=True),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1709738844726
        }
      },
      "id": "5aaaf7f5-ef26-48d8-868d-b53aa4c4f9f4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct the OpenAI Tools agent.\n",
        "> OpenAI API has deprecated functions in favor of tools. The difference between the two is that the tools API allows the model to request that multiple functions be invoked at once, which can reduce response times in some architectures. Itâ€™s recommended to use the tools agent for OpenAI models."
      ],
      "metadata": {},
      "id": "7d527c12-4e18-4f3f-a9ec-8dab4f9ca7b2"
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_openai_tools_agent(llm.with_config(configurable={\"model\": \"gpt35\"}), tools, prompt)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1709738857572
        }
      },
      "id": "6fff2766-defb-45fc-b271-3c811077076b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an agent executor by passing in the agent and tools"
      ],
      "metadata": {},
      "id": "338336d9-a64a-4602-908a-742b418e4520"
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1709738863860
        }
      },
      "id": "ad6c156f-9a17-4daa-80de-70ce2f55063b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Give it memory - since AgentExecutor is also a Runnable class, we do the same with did on Notebook 5"
      ],
      "metadata": {},
      "id": "252a017c-3b36-43ab-8633-78f4f005d166"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_session_history(session_id: str, user_id: str) -> CosmosDBChatMessageHistory:\n",
        "    cosmos = CosmosDBChatMessageHistory(\n",
        "        cosmos_endpoint=os.environ['AZURE_COSMOSDB_ENDPOINT'],\n",
        "        cosmos_database=os.environ['AZURE_COSMOSDB_NAME'],\n",
        "        cosmos_container=os.environ['AZURE_COSMOSDB_CONTAINER_NAME'],\n",
        "        connection_string=os.environ['AZURE_COMOSDB_CONNECTION_STRING'],\n",
        "        session_id=session_id,\n",
        "        user_id=user_id\n",
        "        )\n",
        "\n",
        "    # prepare the cosmosdb instance\n",
        "    cosmos.prepare_cosmos()\n",
        "    return cosmos"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1709738865514
        }
      },
      "id": "7c013314-afe6-4218-b179-d0f7312d2670"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because cosmosDB needs two fields (an id and a partition), and RunnableWithMessageHistory takes by default only one identifier for memory (session_id), we need to use `history_factory_config` parameter and define the multiple keys for the memory class"
      ],
      "metadata": {},
      "id": "13df017f-3ab7-4943-adc1-3477badf3d3e"
    },
    {
      "cell_type": "code",
      "source": [
        "userid_spec = ConfigurableFieldSpec(\n",
        "            id=\"user_id\",\n",
        "            annotation=str,\n",
        "            name=\"User ID\",\n",
        "            description=\"Unique identifier for the user.\",\n",
        "            default=\"\",\n",
        "            is_shared=True,\n",
        "        )\n",
        "session_id = ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"Unique identifier for the conversation.\",\n",
        "            default=\"\",\n",
        "            is_shared=True,\n",
        "        )"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1709738869628
        }
      },
      "id": "bf93758f-da3b-48fb-9882-91fe327b1751"
    },
    {
      "cell_type": "code",
      "source": [
        "agent_with_chat_history = RunnableWithMessageHistory(\n",
        "    agent_executor,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"question\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[userid_spec,session_id]\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1709738870547
        }
      },
      "id": "52d1aaa6-efca-4512-b680-896dae39a359"
    },
    {
      "cell_type": "code",
      "source": [
        "# configure the session id and user id\n",
        "random_session_id = \"session\"+ str(random.randint(1, 1000))\n",
        "ramdom_user_id = \"user\"+ str(random.randint(1, 1000))\n",
        "\n",
        "config={\"configurable\": {\"session_id\": random_session_id, \"user_id\": ramdom_user_id}}\n",
        "config"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "{'configurable': {'session_id': 'session986', 'user_id': 'user57'}}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1709738871529
        }
      },
      "id": "05c6b489-3db9-4965-9eae-ed2790e62bd7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the Agent!"
      ],
      "metadata": {},
      "id": "3295c54e-a5e2-46f6-99fc-6f76453a877d"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent_with_chat_history.invoke({\"question\": \"Hi, I'm Pablo Marin. What's yours\"}, config=config)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "CPU times: user 197 ms, sys: 9.14 ms, total: 206 ms\nWall time: 1.03 s\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "{'question': \"Hi, I'm Pablo Marin. What's yours\",\n 'history': [],\n 'output': \"Hello Pablo Marin, I'm Jarvis, your AI assistant. How can I assist you today?\"}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {},
      "id": "2ac81763-6bcc-4408-9daf-d047a0e2cb08"
    },
    {
      "cell_type": "code",
      "source": [
        "printmd(agent_with_chat_history.invoke(\n",
        "    {\"question\": \"What are markov chains and is there an application in medicine?\"}, \n",
        "    config=config)[\"output\"])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "Markov chains are mathematical models that describe a sequence of events where the probability of each event depends only on the previous event. These models are based on the concept of memorylessness, meaning that the future behavior of the system depends only on its current state and is independent of its past states.\n\nIn the context of medicine, Markov chains have various applications. Here are a few examples:\n\n1. Disease Progression Modeling: Markov chains can be used to model the progression of diseases over time. By dividing the disease progression into discrete states, such as healthy, mild, moderate, and severe, and assigning transition probabilities between these states, healthcare professionals can simulate the progression of the disease and assess the effectiveness of different interventions or treatment strategies.\n\n2. Pharmacokinetics: Markov models are used to study the absorption, distribution, metabolism, and excretion of drugs in the body. These models help in understanding how drugs are processed by the body over time and how their concentrations change in different organs or tissues. This information is crucial for optimizing drug dosing regimens and predicting drug interactions.\n\n3. Epidemiology and Public Health: Markov models are used to study the spread of infectious diseases within a population. By modeling the transitions between different disease states (e.g., susceptible, infected, recovered), researchers can estimate the impact of interventions such as vaccination or quarantine measures on disease transmission and control.\n\n4. Health Economic Modeling: Markov models are widely used in health economics to evaluate the cost-effectiveness of healthcare interventions. These models simulate the natural history of a disease, treatment options, and associated costs over time. By comparing different treatment strategies, policymakers can make informed decisions about resource allocation and healthcare policy.\n\nIt's important to note that while Markov chains provide valuable insights into various medical scenarios, they are simplified representations of complex systems and make certain assumptions about the underlying processes. Therefore, their accuracy and applicability depend on the specific context and the quality of the data used to parameterize the models.\n\nReferences:\n1. [Modeling Infectious Disease Transmission using Markov Chains](https://doi.org/10.1111/ina.12056) - Source 1\n2. [Fast Prediction of Transient Particle Transport in Enclosed Environments using Combined CFD and Markov Chain Method](https://www.ncbi.nlm.nih.gov/pubmed/23789964/) - Source 1\n3. [Fast Prediction of Transient Particle Transport using Combined FFD and Markov Chain Model](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7090511/) - Source 2\n4. [Rapid Mixing of the Face-Reversal Markov Chain on Eulerian Orientations of Solid Subgraphs of the Triangular Lattice](https://datasetsgptsmartsearch.blob.core.windows.net/arxivcs/pdf/0703/0703031v1.pdf) - Source 3"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1709738893112
        }
      },
      "id": "cb3fca7e-33a1-40f1-afb0-dee441a1d1d5"
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    printmd(agent_with_chat_history.invoke(\n",
        "        {\"question\": \"Interesting, Tell me more about the use specifically in the spread of viruses\"},\n",
        "        config=config)[\"output\"])\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "Certainly! Markov chains have been widely used to model the spread of viruses and infectious diseases within a population. These models help researchers and public health officials understand the dynamics of disease transmission and evaluate the effectiveness of various control measures. Here are some key aspects of using Markov chains in the context of virus spread:\n\n1. State Transitions: In a Markov chain model for virus spread, different states represent the possible conditions of individuals within the population, such as susceptible, infected, recovered, or deceased. The transitions between these states are governed by probabilities that depend on factors like contact rates, transmission probabilities, and recovery rates. By modeling these transitions, researchers can simulate the spread of the virus over time.\n\n2. Epidemiological Parameters: Markov chain models require the estimation of epidemiological parameters to accurately represent the virus's behavior. These parameters include the basic reproduction number (R0), which indicates the average number of secondary infections caused by an infected individual in a susceptible population. Other parameters include the duration of the infectious period, the probability of transmission per contact, and the rate of recovery or mortality. Estimating these parameters from available data is crucial for model accuracy.\n\n3. Intervention Strategies: Markov chain models allow researchers to evaluate the impact of different intervention strategies on virus spread. By modifying the transition probabilities in the model, such as reducing contact rates or increasing the effectiveness of preventive measures like vaccination or social distancing, researchers can assess the potential effectiveness of these interventions in reducing the spread of the virus.\n\n4. Sensitivity Analysis: Markov chain models can be used to perform sensitivity analyses to understand the impact of uncertainties in parameter values on model outcomes. By varying the parameter values within plausible ranges, researchers can assess the robustness of their findings and identify key factors that influence the spread of the virus.\n\nIt's important to note that Markov chain models provide a simplified representation of virus spread and make certain assumptions about the population dynamics and transmission processes. Real-world virus spread is influenced by various complex factors, including population demographics, behavior, and spatial dynamics. Therefore, these models are most effective when used in combination with other modeling approaches and empirical data to inform public health decision-making.\n\nReferences:\n1. [Modeling Infectious Disease Transmission using Markov Chains](https://doi.org/10.1111/ina.12056) - Source 1\n2. [Fast Prediction of Transient Particle Transport in Enclosed Environments using Combined CFD and Markov Chain Method](https://www.ncbi.nlm.nih.gov/pubmed/23789964/) - Source 1\n3. [Fast Prediction of Transient Particle Transport using Combined FFD and Markov Chain Model](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7090511/) - Source 2\n4. [Rapid Mixing of the Face-Reversal Markov Chain on Eulerian Orientations of Solid Subgraphs of the Triangular Lattice](https://datasetsgptsmartsearch.blob.core.windows.net/arxivcs/pdf/0703/0703031v1.pdf) - Source 3"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1709738906880
        }
      },
      "id": "c430c456-f390-4319-a3b1-bee19da130cf"
    },
    {
      "cell_type": "code",
      "source": [
        "printmd(agent_with_chat_history.invoke({\"question\": \"Thhank you!\"}, config=config)[\"output\"])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "You're welcome! If you have any more questions, feel free to ask. I'm here to help!"
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1709738915034
        }
      },
      "id": "9fd54f71-03c9-4332-885b-0d1df942fa88"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Important: there is a limitation of GPT3.5, once we start adding long prompts, and long contexts and thorough answers, or the agent makes multiple searches for multi-step questions, we run out of space!\n",
        "\n",
        "You can minimize this by:\n",
        "- Shorter System Prompt\n",
        "- Smaller chunks (less than the default of 5000 characters)\n",
        "- Reducing topK to bring less relevant chunks\n",
        "\n",
        "However, you ultimately are sacrificing quality to make everything work with GPT3.5 (cheaper and faster model)"
      ],
      "metadata": {},
      "id": "149648ba-945d-4e7d-81f7-a8bca2ac87f2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's add more things we have learned so far: dynamic LLM selection of GPT4 and asyncronous streaming"
      ],
      "metadata": {},
      "id": "41787714-73fd-4336-85f2-bec3abb41eda"
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_openai_tools_agent(llm.with_config(configurable={\"model\": \"gpt4\"}), tools, prompt) # We select now GPT-4\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)\n",
        "agent_with_chat_history = RunnableWithMessageHistory(agent_executor,get_session_history,input_messages_key=\"question\", \n",
        "                                                     history_messages_key=\"history\", history_factory_config=[userid_spec,session_id])"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1709738928747
        }
      },
      "id": "1511d2c3-97fe-4232-a560-014d0f157008"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In prior notebooks with use the function `.stream()` of the runnable in order to stream the tokens. However if you need to stream individual tokens from the agent or surface steps occuring within tools, you would need to use a combination of `Callbacks` and `.astream()` OR the new `astream_events` API (beta).\n",
        "\n",
        "Letâ€™s use here the astream_events API to stream the following events:\n",
        "\n",
        "    Agent Start with inputs\n",
        "    Tool Start with inputs\n",
        "    Tool End with outputs\n",
        "    Stream the agent final anwer token by token\n",
        "    Agent End with outputs"
      ],
      "metadata": {},
      "id": "7bec5b32-6017-44b9-97e7-34ba3695e688"
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION = \"Tell me more about your last answer, search again multiple times and provide a deeper explanation\""
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1709738931035
        }
      },
      "id": "9600a35e-8d2e-43d0-a334-092b2e8b832c"
    },
    {
      "cell_type": "code",
      "source": [
        "async for event in agent_with_chat_history.astream_events(\n",
        "    {\"question\": QUESTION}, config=config, version=\"v1\",\n",
        "):\n",
        "    kind = event[\"event\"]\n",
        "    if kind == \"on_chain_start\":\n",
        "        if (\n",
        "            event[\"name\"] == \"AgentExecutor\"\n",
        "        ):  # Was assigned when creating the agent with `.with_config({\"run_name\": \"Agent\"})`\n",
        "            print(\n",
        "                f\"Starting agent: {event['name']}\"\n",
        "            )\n",
        "    elif kind == \"on_chain_end\":\n",
        "        if (\n",
        "            event[\"name\"] == \"AgentExecutor\"\n",
        "        ):  # Was assigned when creating the agent with `.with_config({\"run_name\": \"Agent\"})`\n",
        "            print()\n",
        "            print(\"--\")\n",
        "            print(\n",
        "                f\"Done agent: {event['name']}\"\n",
        "            )\n",
        "    if kind == \"on_chat_model_stream\":\n",
        "        content = event[\"data\"][\"chunk\"].content\n",
        "        if content:\n",
        "            # Empty content in the context of OpenAI means\n",
        "            # that the model is asking for a tool to be invoked.\n",
        "            # So we only print non-empty content\n",
        "            print(content, end=\"\")\n",
        "    elif kind == \"on_tool_start\":\n",
        "        print(\"--\")\n",
        "        print(\n",
        "            f\"Starting tool: {event['name']} with inputs: {event['data'].get('input')}\"\n",
        "        )\n",
        "    elif kind == \"on_tool_end\":\n",
        "        print(f\"Done tool: {event['name']}\")\n",
        "        # print(f\"Tool output was: {event['data'].get('output')}\")\n",
        "        print(\"--\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Starting agent: AgentExecutor\n--\nStarting tool: docsearch with inputs: {'query': 'Markov chains in virus spread'}\n--\nStarting tool: docsearch with inputs: {'query': 'Markov models epidemiology'}\n--\nStarting tool: docsearch with inputs: {'query': 'Markov chains infectious diseases'}\nDone tool: docsearch\n--\nDone tool: docsearch\n--\nDone tool: docsearch\n--\nMarkov chain models provide a framework for understanding the spread of viruses by representing the transmission of infection as a series of probabilistic events. Here's a deeper explanation of how these models can be applied to the spread of viruses, with a focus on the specific studies and methods mentioned in the documents:\n\n1. **Spatial Markov Chain Model for Virus Spread**:\n   - A spatial Markov chain model represents individuals as nodes in a graph, with connections (vertices) between nodes representing relationships between people. The likelihood of virus transmission from one person to another depends on the intensity of their contact and is determined by chance. This model can be extended to include different lockdown scenarios, allowing for the study of how social distancing measures might influence the spread of a virus<sup><a href=\"https://arxiv.org/pdf/2004.05635v1.pdf\" target=\"_blank\">[1]</a></sup>.\n\n2. **Stochastic Epidemic Models with Two Groups**:\n   - In the context of emerging diseases, the concept of superspreadersâ€”highly infectious individuals who infect a large number of susceptible individualsâ€”is critical. A continuous-time Markov chain (CTMC) model is used to study disease emergence or re-emergence from different groups, where transmission rates may depend on either the infectious host or the susceptible host. The probability of a major epidemic is greater if initiated by a superspreader or by an individual from a highly susceptible group<sup><a href=\"https://doi.org/10.1080/17513758.2018.1538462\" target=\"_blank\">[2]</a></sup>.\n\n3. **Combined Computational Fluid Dynamics (CFD) and Markov Chain Method**:\n   - This method calculates a transition probability matrix using CFD simulations and then applies the Markov chain technique to predict transient particle concentration distributions in enclosed environments. It is validated in various scenarios, such as clean rooms and airplane cabins, and can provide faster-than-real-time information about particle transport, which is essential for assessing infection risk to occupants<sup><a href=\"https://doi.org/10.1111/ina.12056\" target=\"_blank\">[3]</a></sup>.\n\n4. **Nonlinear Markov Chains Model for COVID-19 Pandemic Analysis**:\n   - A nonlinear Markov chains model is proposed to analyze and understand the behavior of the COVID-19 pandemic. Using data from China, the model estimates the daily new COVID-19 cases in several countries and examines the correlation between daily new cases and the number of deaths<sup><a href=\"http://medrxiv.org/cgi/content/short/2020.04.21.20073668v1\" target=\"_blank\">[4]</a></sup>.\n\n5. **Bayesian Markov Chain Monte Carlo-Based Inference**:\n   - A new method for Bayesian Markov Chain Monte Carlo-based inference in stochastic models is presented, suitable for modeling noisy epidemic data. It applies the uniformization representation of a Markov process to efficiently generate appropriate conditional distributions in the Gibbs sampler algorithm, particularly useful in data-poor settings<sup><a href=\"https://doi.org/10.1093/biostatistics/kxr019\" target=\"_blank\">[5]</a></sup>.\n\nEach of these methods and models offers unique insights into the dynamics of virus spread and the effectiveness of intervention strategies. They underscore the importance of considering both individual behaviors (such as superspreading events) and broader population dynamics (such as lockdown measures) in understanding and controlling epidemics. These models also highlight the need for accurate data on transmission rates, contact patterns, and the effectiveness of public health interventions to inform and refine the models for better prediction and management of infectious disease outbreaks.\n--\nDone agent: AgentExecutor\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1709739119525
        }
      },
      "id": "3808fa33-05bb-4f5d-9ab9-7159f6db62a8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Note: Try to run this last question with GPT3.5 and see how you are going to run out of token space in the LLM"
      ],
      "metadata": {},
      "id": "4b41bba7-18df-4ab8-b4f6-60368160d348"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "We just built our first RAG BOT!.\n",
        "\n",
        "- We learned that **Agents + Tools are the best way to go about building Bots**. <br>\n",
        "- We converted the Azure Search retriever into a Tool using the function `GetDocSearchResults_Tool` in `utils.py`\n",
        "- We learned about the events API (Beta), one way to stream the answer from agents\n",
        "- We learned that for comprehensive, quality answers we will run out of space with GPT3.5. GPT4 then becomes necessary.\n"
      ],
      "metadata": {},
      "id": "e0ec64bf-fe24-42fc-8dde-4d478f0af21e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEXT\n",
        "\n",
        "Now that we have a bot with one skill (Document Search), let's build more skills!. In the next Notebook, we are going to build an agent that can understand tabular data in csv file and can execute python commands"
      ],
      "metadata": {},
      "id": "56306506-d53d-4d43-93e2-a9300ed2a3ee"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}